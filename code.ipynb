{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_data(data, file_path):\n",
    "    \"\"\"Save data to a JSON file.\"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "def read_data(file_path):\n",
    "    \"\"\"Read data from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# streams = read_data('./1_flows_data.json')\n",
    "# filtered_streams = read_data('./2_filtered_flows_data.json')\n",
    "# occurrences = read_data('./3-occurrences_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import sniff, IP, TCP\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "WAITING_TIME = 30  # Time threshold to consider packets part of the same flow\n",
    "\n",
    "\n",
    "def create_flow_key_from_packet(packet):\n",
    "    \"\"\"Generate a unique flow key based on packet IP and TCP headers.\"\"\"\n",
    "    return f\"{packet[IP].src}:{packet[TCP].sport}-{packet[IP].dst}:{packet[TCP].dport}\"\n",
    "\n",
    "def extract_packet_info(packet):\n",
    "    \"\"\"Extract necessary information from the packet.\"\"\"\n",
    "    return {\n",
    "        'src_ip': packet[IP].src,\n",
    "        'src_port': packet[TCP].sport,\n",
    "        'dst_ip': packet[IP].dst,\n",
    "        'dst_port': packet[TCP].dport,\n",
    "        'protocol': packet[IP].proto,\n",
    "        'time': float(packet.time),  # Convert to float\n",
    "        'size': len(packet)\n",
    "    }\n",
    "\n",
    "def update_or_create_flow(flows, flow_key, packet_info):\n",
    "    \"\"\"Update an existing flow or create a new one based on the WAITING_TIME.\"\"\"\n",
    "    current_time = packet_info['time']\n",
    "    if flow_key in flows:\n",
    "        last_flow = flows[flow_key][-1]\n",
    "        if current_time - last_flow['end_time'] < WAITING_TIME:\n",
    "            last_flow['packets'].append(packet_info)\n",
    "            last_flow['end_time'] = current_time\n",
    "            last_flow['total_bytes'] += packet_info['size']\n",
    "        else:\n",
    "            new_flow = {\n",
    "                'packets': [packet_info],\n",
    "                'start_time': current_time,\n",
    "                'end_time': current_time,\n",
    "                'total_bytes': packet_info['size'],\n",
    "                'src_ip': packet_info['src_ip'],\n",
    "                'dst_ip': packet_info['dst_ip'],\n",
    "                'protocol': packet_info['protocol']\n",
    "            }\n",
    "            flows[flow_key].append(new_flow)\n",
    "    else:\n",
    "        flows[flow_key] = [{\n",
    "            'packets': [packet_info],\n",
    "            'start_time': current_time,\n",
    "            'end_time': current_time,\n",
    "            'total_bytes': packet_info['size'],\n",
    "            'src_ip': packet_info['src_ip'],\n",
    "            'dst_ip': packet_info['dst_ip'],\n",
    "            'protocol': packet_info['protocol']\n",
    "        }]\n",
    "    return flows\n",
    "\n",
    "def manage_flow(packet, flows):\n",
    "    \"\"\"Process each packet to manage flow data.\"\"\"\n",
    "    if IP in packet and TCP in packet:\n",
    "        flow_key = create_flow_key_from_packet(packet)\n",
    "        packet_info = extract_packet_info(packet)\n",
    "        flows = update_or_create_flow(flows, flow_key, packet_info)\n",
    "    return flows\n",
    "\n",
    "def read_pcap_file(pcap_file):\n",
    "    \"\"\"Read packets from a pcap file and manage flows.\"\"\"\n",
    "    flows = {}\n",
    "    def packet_processor(packet):\n",
    "        nonlocal flows\n",
    "        flows = manage_flow(packet, flows)\n",
    "    sniff(offline=pcap_file, prn=packet_processor, store=False)\n",
    "    return flows\n",
    "\n",
    "# Example usage\n",
    "pcap_file = 'EX-3.pcap'\n",
    "flows = read_pcap_file(pcap_file)\n",
    "save_data(flows, './1_flows_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for filtering\n",
    "MAX_BYTES_THRESHOLD = 5000  # Maximum bytes to consider a flow as relevant\n",
    "MAX_DURATION_THRESHOLD = 300  # Maximum duration in seconds to consider a flow as relevant\n",
    "MIN_PACKET_COUNT_THRESHOLD = 5  # Minimum packet count to consider a flow as relevant\n",
    "\n",
    "def filter_flows(flows):\n",
    "    \"\"\"Filter flows based on size, duration, and packet count.\"\"\"\n",
    "    filtered_flows = {}\n",
    "    for flow_key, flow_list in flows.items():\n",
    "        filtered_flow_list = []\n",
    "        for flow in flow_list:\n",
    "            total_bytes = flow['total_bytes']\n",
    "            duration = flow['end_time'] - flow['start_time']\n",
    "            packet_count = len(flow['packets'])\n",
    "\n",
    "            # Filter based on the defined thresholds\n",
    "            if (total_bytes <= MAX_BYTES_THRESHOLD and \n",
    "                duration <= MAX_DURATION_THRESHOLD and \n",
    "                packet_count >= MIN_PACKET_COUNT_THRESHOLD):\n",
    "                filtered_flow_list.append(flow)\n",
    "        \n",
    "        if filtered_flow_list:\n",
    "            filtered_flows[flow_key] = filtered_flow_list\n",
    "    \n",
    "    return filtered_flows\n",
    "\n",
    "# Example usage\n",
    "filtered_flows = filter_flows(flows)\n",
    "save_data(filtered_flows, './2_filtered_flows_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "T_dep = 120  # حداکثر فاصله زمانی مجاز بین دو جریان برای احتمال وابستگی (ثانیه)\n",
    "N_dep = 10   # حداکثر تفاوت در تعداد تکرار بین دو جریان\n",
    "Sdep_th = 0.5  # حداقل امتیاز وابستگی برای تشخیص به عنوان وابستگی واقعی\n",
    "\n",
    "def compute_occurrences(flows):\n",
    "    \"\"\"Compute the number of occurrences for each flow.\"\"\"\n",
    "    occurrences = {}\n",
    "    for flow_key, sessions in flows.items():\n",
    "        occurrences[flow_key] = sum(len(session['packets']) for session in sessions)\n",
    "    return occurrences\n",
    "\n",
    "flow_occurrences = compute_occurrences(filtered_flows)\n",
    "save_data(flow_occurrences, './3_flow_occurrences_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'flow_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_dependencies\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m \u001b[43mextract_two_level_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_dep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_dep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSdep_th\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# print(\"Extracted Dependencies:\")\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# for dep, score in dependencies.items():\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#     print(f\"Dependency between {dep[0]} and {dep[1]} with score {score}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 20\u001b[0m, in \u001b[0;36mextract_two_level_dependencies\u001b[1;34m(flows, T_dep, N_dep, Sdep_th)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Check the time gap condition\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (fj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m fi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m T_dep:\n\u001b[1;32m---> 20\u001b[0m     flow_key_i \u001b[38;5;241m=\u001b[39m \u001b[43mfi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflow_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m     flow_key_j \u001b[38;5;241m=\u001b[39m fj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_key\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m     Ni \u001b[38;5;241m=\u001b[39m occurrences[flow_key_i]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'flow_key'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def extract_two_level_dependencies(flows, T_dep, N_dep, Sdep_th):\n",
    "    \"\"\"Extract two-level flow dependencies based on the specified thresholds.\"\"\"\n",
    "    occurrences = compute_occurrences(flows)\n",
    "    dependencies = {}\n",
    "\n",
    "    # Sort all flows by start time within each host\n",
    "    for host, sessions in flows.items():\n",
    "        sorted_flows = sorted(sessions, key=lambda x: x['start_time'])\n",
    "\n",
    "        # Compare each pair of flows within the sorted list\n",
    "        for i in range(len(sorted_flows)):\n",
    "            for j in range(i + 1, len(sorted_flows)):\n",
    "                fi = sorted_flows[i]\n",
    "                fj = sorted_flows[j]\n",
    "\n",
    "                # Check the time gap condition\n",
    "                if (fj['start_time'] - fi['end_time']) <= T_dep:\n",
    "                    flow_key_i = fi['flow_key']\n",
    "                    flow_key_j = fj['flow_key']\n",
    "                    Ni = occurrences[flow_key_i]\n",
    "                    Nj = occurrences[flow_key_j]\n",
    "\n",
    "                    # Check the occurrence difference condition\n",
    "                    if abs(Ni - Nj) <= N_dep:\n",
    "                        dependency_key = (flow_key_i, flow_key_j)\n",
    "                        if dependency_key in dependencies:\n",
    "                            dependencies[dependency_key] += 1\n",
    "                        else:\n",
    "                            dependencies[dependency_key] = 1\n",
    "\n",
    "    # Calculate the dependency score and filter by threshold\n",
    "    final_dependencies = {}\n",
    "    for (fi, fj), Tij in dependencies.items():\n",
    "        Ni = occurrences[fi]\n",
    "        Nj = occurrences[fj]\n",
    "        Sdep = math.sqrt(Tij ** 2 / (Ni * Nj))\n",
    "        if Sdep > Sdep_th:\n",
    "            final_dependencies[(fi, fj)] = Sdep\n",
    "\n",
    "    return final_dependencies\n",
    "\n",
    "# Example usage\n",
    "dependencies = extract_two_level_dependencies(flows, T_dep, N_dep, Sdep_th)\n",
    "\n",
    "# print(\"Extracted Dependencies:\")\n",
    "# for dep, score in dependencies.items():\n",
    "#     print(f\"Dependency between {dep[0]} and {dep[1]} with score {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
