{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_data(data, file_path):\n",
    "    \"\"\"Save data to a JSON file.\"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "def read_data(file_path):\n",
    "    \"\"\"Read data from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_flows_to_json(flows, filename):\n",
    "    \"\"\"Saves flow data to a JSON file.\"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(flows, file, indent=4)\n",
    "        # json.dump(flows, file)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read packets from file and make flows from all packets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No libpcap provider available ! pcap won't be used\n"
     ]
    }
   ],
   "source": [
    "from scapy.all import rdpcap, IP, TCP, UDP\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def create_flow_key(packet):\n",
    "    \"\"\"Create a normalized flow key for bi-directional traffic.\"\"\"\n",
    "    if IP in packet:\n",
    "        src_ip, dst_ip = packet[IP].src, packet[IP].dst\n",
    "        src_port, dst_port = (packet[TCP].sport, packet[TCP].dport) if TCP in packet else (packet[UDP].sport, packet[UDP].dport) if UDP in packet else (0, 0)\n",
    "        if (src_ip > dst_ip) or (src_ip == dst_ip and src_port > dst_port):\n",
    "            src_ip, dst_ip = dst_ip, src_ip\n",
    "            src_port, dst_port = dst_port, src_port\n",
    "        protocol = packet[IP].proto\n",
    "        return f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}_proto_{protocol}\"\n",
    "    return None\n",
    "\n",
    "def process_pcap(file_path, time_threshold=5):\n",
    "    \"\"\"Process packets from a pcap file and organize them into flows and sessions.\"\"\"\n",
    "    packets = rdpcap(file_path)\n",
    "    flows = {}\n",
    "\n",
    "    for packet in packets:\n",
    "        if IP in packet and (TCP in packet or UDP in packet):\n",
    "            key = create_flow_key(packet)\n",
    "            # print(1)\n",
    "            if key:\n",
    "                if key not in flows:\n",
    "                    flows[key] = {\n",
    "                        'start_time': float(\"{:.2f}\".format(packet.time)),\n",
    "                        'end_time': float(\"{:.2f}\".format(packet.time)),\n",
    "                        'total_size': len(packet),\n",
    "                        'sessions': [{\n",
    "                            'src_ip': packet[IP].src,\n",
    "                            'dst_ip': packet[IP].dst,\n",
    "                            'src_port': packet[TCP].sport if TCP in packet else packet[UDP].sport,\n",
    "                            'dst_port': packet[TCP].dport if TCP in packet else packet[UDP].dport,\n",
    "                            'protocol': packet[IP].proto,\n",
    "                            'start_time': float(\"{:.2f}\".format(packet.time)),\n",
    "                            'end_time': float(\"{:.2f}\".format(packet.time)),\n",
    "                            'total_size': len(packet),\n",
    "                            'number_of_packets': 1\n",
    "                            # 'packet_summaries': [packet.summary()]  # Store summaries instead of raw packets\n",
    "                        }]\n",
    "                    }\n",
    "                else:\n",
    "                    flow = flows[key]\n",
    "                    last_session = flow['sessions'][-1]\n",
    "                    if float(\"{:.2f}\".format(packet.time)) - last_session['end_time'] > time_threshold:\n",
    "                        flow['sessions'].append({\n",
    "                            'src_ip': packet[IP].src,\n",
    "                            'dst_ip': packet[IP].dst,\n",
    "                            'src_port': packet[TCP].sport if TCP in packet else packet[UDP].sport,\n",
    "                            'dst_port': packet[TCP].dport if TCP in packet else packet[UDP].dport,\n",
    "                            'protocol': packet[IP].proto,\n",
    "                            'start_time': float(\"{:.2f}\".format(packet.time)),\n",
    "                            'end_time': float(\"{:.2f}\".format(packet.time)),\n",
    "                            'total_size': len(packet),\n",
    "                            'number_of_packets': 1\n",
    "                            # 'packet_summaries': [packet.summary()]  # Store summaries instead of raw packets\n",
    "                        })\n",
    "                    else:\n",
    "                        last_session['end_time'] = float(\"{:.2f}\".format(packet.time))\n",
    "                        last_session['total_size'] += len(packet)\n",
    "                        last_session['number_of_packets'] += 1\n",
    "                        \n",
    "                    flow['end_time'] = float(\"{:.2f}\".format(packet.time))\n",
    "                    flow['total_size'] += len(packet)\n",
    "\n",
    "    return flows\n",
    "\n",
    "\n",
    "\n",
    "flows = process_pcap('EX-3.pcap')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save all flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flows_to_json(flows, '1_flows.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = read_data('1_flows.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define thresholds\n",
    "SIZE_THRESHOLD = 5000  # Example threshold for size (bytes)\n",
    "DURATION_THRESHOLD = 300  # Example threshold for duration (seconds)\n",
    "MIN_OCCURRENCE_THRESHOLD = 3  # Minimum occurrences (sessions per flow)\n",
    "\n",
    "def filter_sessions(flows):\n",
    "    \"\"\"\n",
    "    Filters out sessions that:\n",
    "    - Have a total size greater than `size_threshold`.\n",
    "    - Have a duration longer than `duration_threshold`.\n",
    "    - Occur less frequently than `min_occurrence_threshold`.\n",
    "    \"\"\"\n",
    "    filtered_flows = {}\n",
    "\n",
    "    for flow_key, flow_data in flows.items():\n",
    "        filtered_sessions = []\n",
    "        for session in flow_data['sessions']:\n",
    "            duration = session['end_time'] - session['start_time']\n",
    "            if session['total_size'] <= SIZE_THRESHOLD and duration <= DURATION_THRESHOLD:\n",
    "                filtered_sessions.append(session)\n",
    "\n",
    "        # Only include flows with enough sessions\n",
    "        if len(filtered_sessions) >= MIN_OCCURRENCE_THRESHOLD:\n",
    "            filtered_flows[flow_key] = flow_data.copy()\n",
    "            # filtered_flows[flow_key]['sessions'] = filtered_sessions\n",
    "\n",
    "    return filtered_flows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_flows = filter_sessions(flows)\n",
    "save_flows_to_json(filtered_flows, '2-filtered_flows.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_flows = read_data('2-filtered_flows.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_occurrences(flows):\n",
    "    \"\"\"Count occurrences of each flow and store the count.\"\"\"\n",
    "    occurrences = {}\n",
    "    for flow_key, flow_data in flows.items():\n",
    "        # occurrences[flow_key] = sum(session['number_of_packets'] for session in flow_data['sessions'])\n",
    "        occurrences[flow_key] = len(flow_data['sessions'])\n",
    "    return occurrences\n",
    "\n",
    "occurrences = compute_occurrences(filtered_flows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def extract_dependencies(flows, T_dep, N_dep, S_dep_th):\n",
    "    \"\"\"Extract two-level dependencies based on temporal proximity and occurrence similarity.\"\"\"\n",
    "    occurrences = compute_occurrences(flows)\n",
    "    dependencies = {}\n",
    "    Sdep_scores = {}\n",
    "\n",
    "    # Prepare flows for processing by sorting them based on the start time of their sessions\n",
    "    for flow_key, flow_data in flows.items():\n",
    "        flow_data['sessions'].sort(key=lambda x: x['start_time'])\n",
    "\n",
    "    # Compare each flow with every other flow\n",
    "    for fi_key, fi_data in flows.items():\n",
    "        for fj_key, fj_data in flows.items():\n",
    "            if fi_key != fj_key:\n",
    "                for fi_session in fi_data['sessions']:\n",
    "                    for fj_session in fj_data['sessions']:\n",
    "                        if abs(fi_session['start_time'] - fj_session['start_time']) <= T_dep:\n",
    "                            Ni = occurrences[fi_key]\n",
    "                            Nj = occurrences[fj_key]\n",
    "                            if abs(Ni - Nj) < N_dep:\n",
    "                                pair_key = (fi_key, fj_key)\n",
    "                                if pair_key in dependencies:\n",
    "                                    dependencies[pair_key] += 1\n",
    "                                else:\n",
    "                                    dependencies[pair_key] = 1\n",
    "\n",
    "    # Calculate Sdep scores for all identified dependencies\n",
    "    for (fi, fj), Tij in dependencies.items():\n",
    "        Ni = occurrences[fi]\n",
    "        Nj = occurrences[fj]\n",
    "        Sdep = math.sqrt(Tij**2 / (Ni * Nj))\n",
    "        if Sdep > S_dep_th:\n",
    "            Sdep_scores[f\"{fi}, {fj}\"] = Sdep\n",
    "\n",
    "    return Sdep_scores\n",
    "\n",
    "# Define thresholds\n",
    "T_dep = 30  # Maximum time difference between flow starts\n",
    "N_dep = 5   # Maximum difference in occurrences\n",
    "S_dep_th = 0.5  # Minimum score threshold for a dependency to be considered significant\n",
    "\n",
    "# Assuming `flows` is your data structure loaded from somewhere as described\n",
    "dependencies = extract_dependencies(filtered_flows, T_dep, N_dep, S_dep_th)\n",
    "# print(\"Dependencies with scores:\", dependencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flows_to_json(occurrences, '3-1-occurrences.json')\n",
    "\n",
    "save_flows_to_json(dependencies, '3-2-dependencies.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = read_data('3-2-dependencies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dependencies(dependencies):\n",
    "    \"\"\"Parse the dependencies to a more accessible structure.\"\"\"\n",
    "    parsed_dependencies = {}\n",
    "    for key, score in dependencies.items():\n",
    "        flows = key.split(\", \")\n",
    "        for i in range(len(flows) - 1):\n",
    "            if flows[i] not in parsed_dependencies:\n",
    "                parsed_dependencies[flows[i]] = []\n",
    "            parsed_dependencies[flows[i]].append((flows[i + 1], score))\n",
    "    return parsed_dependencies\n",
    "\n",
    "def find_multi_layer_dependencies(parsed_dependencies):\n",
    "    \"\"\"Construct multi-layer dependencies from two-layer dependencies.\"\"\"\n",
    "    multi_layer_dependencies = {}\n",
    "    \n",
    "    for source_flow, targets in parsed_dependencies.items():\n",
    "        for target_flow, score in targets:\n",
    "            if target_flow in parsed_dependencies:  # Check if the target has further dependencies\n",
    "                for next_target, next_score in parsed_dependencies[target_flow]:\n",
    "                    multi_layer_key = f\"{source_flow}, {target_flow}, {next_target}\"\n",
    "                    multi_layer_dependencies[multi_layer_key] = min(score, next_score)  # Use the min score as the dependency strength\n",
    "    \n",
    "    return multi_layer_dependencies\n",
    "\n",
    "# Example data\n",
    "\n",
    "parsed_dependencies = parse_dependencies(dependencies)\n",
    "multi_layer_dependencies = find_multi_layer_dependencies(parsed_dependencies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_layer_dependencies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m save_flows_to_json(parsed_dependencies, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4-1-parsed_dependencies.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m save_flows_to_json(\u001b[43mmulti_layer_dependencies\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4-2-multi_layer_dependencies.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multi_layer_dependencies' is not defined"
     ]
    }
   ],
   "source": [
    "save_flows_to_json(parsed_dependencies, '4-1-parsed_dependencies.json')\n",
    "save_flows_to_json(multi_layer_dependencies, '4-2-multi_layer_dependencies.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
